# Cloud Data Warehouses

## Introduction

Implementation of a Data Warehouse in the AWS. For this purpose, the database is an AWS Redshift. The data is stored in an Amazon S3 in a JSON format. The data must be loaded from the S3 to two staging tables in Amazon Redshift and then to a Star Schema in the same Database. With the star schema, the company Sparkify will be able to easily analyze its data using the advantages of a Cloud solution. 

## Project Dataset

The AWS S3 contains metadata about a song and the artist of that song and the logs generated by a simulator based on the songs in the prior dataset.

Here are the S3 links for each:
Song data: s3://udacity-dend/song_data
Log data: s3://udacity-dend/log_data
Log data JSON path: s3://udacity-dend/log_json_path.json

## Project Steps

1 - Design schemas for your fact and dimension tables;
2 - Create a Redshift Cluster in the  AWS;
3 - Configure the 'dwh.cfg' file;
4 - Write the scripts for creating and dropping the staging tables and the tables from the star schema;
5 - Write the script to load data from the SE Bucket to the staging tables using the COPY command;
6 - Write the script to load data from the staging tables to the star schema transforming and treating the data.

## Result

After all the steps are complete, check the data in the AWS console. For example:

Find all the artists from NY in  the database: `select artist_id, name  from artist where location = 'New York, NY';`

Find the users that use the Paid level of subscription: `select user_id, first_name, last_name, gender, level FROM users WHERE level = 'paid';`